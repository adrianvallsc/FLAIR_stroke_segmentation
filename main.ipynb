{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrianvallsc/FLAIR_stroke_segmentation/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nnUNET para la segmentación de imágenes FLAIR de ictus\n",
        "\n",
        "En este proyecto intentaremos entrenar una nnUNet para la segmentación automática de imágenes en FLAIR.\n",
        "\n",
        "nnUNET es un paradigma de redes que permite el entrenamiento de redes profundas con arquitectura U-Net, la cual se ha demostrado ser de gran utilidad a la hora de segmentar imágenes médicas.\n",
        "\n",
        "Para el entrenamiento de estas redes haremos uso GPUs como recursos computacionales. El actual cuaderno está pensado para ser ejecutado en Google Colab, aunque puede adapatarse para otros recursos\n"
      ],
      "metadata": {
        "id": "GjywUw-uGdzP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpPkxClEr_G6",
        "outputId": "7b103c10-5565-4a38-815c-0ad0d17ec40a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Montamos google Colab, donde ya tenemos los datos organizados\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalación de nnUNet\n",
        "\n",
        "Para la instalación de nnUNet, en primer lugar debemos instalar el paquete. En mi caso, el paquete lo tengo descargado dentro de mi Drive, por lo que no necesito descargarlo"
      ],
      "metadata": {
        "id": "mRuNTS0FJPE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Necesitamos definir una serie de paths\n",
        "\n",
        "# Base path = donde se almacenan las imágenes\n",
        "# nnUNetFrame = donde se guardarán todos los modelos\n",
        "# Debe de tener la siguiente estructura\n",
        "# nnUNetFrame\n",
        "# ├── dataset\n",
        "#   ├── nnUNet_raw\n",
        "#   ├── nnUNet_preprocessed\n",
        "#   ├── nnUNet_trained_models\n",
        "\n",
        "# Modificar según interese en el\n",
        "wd = \"/content/drive/MyDrive\"\n",
        "foldername = \"Dataset001_Stroke\"\n",
        "\n",
        "nnFrame = os.path.join(wd, \"nnUNetFrame\")\n",
        "nnDataset = os.path.join(nnFrame, \"dataset\")\n",
        "\n",
        "# Configuración de variables de entorno\n",
        "os.environ['nnUNet_raw'] = os.path.join(nnDataset, \"nnUNet_raw\")\n",
        "os.environ['nnUNet_preprocessed'] = os.path.join(nnDataset, \"nnUNet_preprocessed\")\n",
        "os.environ['nnUNet_results'] = os.path.join(nnDataset, \"nnUNet_trained_models\")\n",
        "os.environ[\"base_path\"] = os.path.join(wd, \"segmentation\")\n",
        "os.environ[\"work_nnUNet\"] = wd\n",
        "\n",
        "\n",
        "# Rutas específicas del dataset\n",
        "\n",
        "\n",
        "dataStroke = os.path.join(os.environ['nnUNet_raw'], \"Dataset001_Stroke\")\n",
        "dataLabels = os.path.join(dataStroke, \"labelsTr\")\n",
        "dataImages = os.path.join(dataStroke, \"imagesTr\")\n",
        "\n",
        "# Función para crear directorios si no existen\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "# Creación de directorios necesarios\n",
        "directories = [\n",
        "    os.environ['nnUNet_raw'],\n",
        "    os.environ['nnUNet_preprocessed'],\n",
        "    os.environ['nnUNet_results'],\n",
        "    dataStroke,\n",
        "    dataLabels,\n",
        "    dataImages\n",
        "]\n",
        "\n",
        "for directory in directories:\n",
        "    create_dir(directory)"
      ],
      "metadata": {
        "id": "f5Vx3rnBLfhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Posteriormente será necesaria la instalación del paquete nnUNet. De forma estándar el paquete entrena 1000 épocas cada uno de los modelos, lo cual requiere una alta disponibilidad de recursos computacionales. Por este motivo es necesario limitar dichos requerimientos, a pesar de que esto pueda suponer un empeoramiento en las capacidades del modelo.\n",
        "\n",
        "Para hacerlo, una vez que hemos descargado el paquete mediante el comando\n",
        "\n",
        "```\n",
        "git clone https://github.com/MIC-DKFZ/nnUNet.git\n",
        "```\n",
        "\n",
        "Debemos entrar en `nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py` y modificar el parámetro `self.num_epochs = 1000` de la linea 152 para reducirlo al número solicitado. En mi caso lo he reducido a 200 épocas\n"
      ],
      "metadata": {
        "id": "AyOSatsuPaJu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNjiebtF1GuZ",
        "outputId": "2f95297a-20e8-4f5c-941f-9ca5e2d4b067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/drive/MyDrive/nnUNetFrame/nnUNet\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Checking if build backend supports build_editable: started\n",
            "  Checking if build backend supports build_editable: finished with status 'done'\n",
            "  Getting requirements to build editable: started\n",
            "  Getting requirements to build editable: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing editable metadata (pyproject.toml): started\n",
            "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: torch>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.4.2) (2.3.0+cu121)\n",
            "Collecting acvl-utils<0.3,>=0.2 (from nnunetv2==2.4.2)\n",
            "  Downloading acvl_utils-0.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting dynamic-network-architectures<0.4,>=0.3.1 (from nnunetv2==2.4.2)\n",
            "  Downloading dynamic_network_architectures-0.3.1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.4.2) (4.66.4)\n",
            "Collecting dicom2nifti (from nnunetv2==2.4.2)\n",
            "  Downloading dicom2nifti-2.4.11-py3-none-any.whl (43 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 3.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.4.2) (1.11.4)\n",
            "Collecting batchgenerators>=0.25 (from nnunetv2==2.4.2)\n",
            "  Downloading batchgenerators-0.25.tar.gz (61 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.7/61.7 kB 5.9 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.4.2) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.4.2) (1.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.4.2) (0.19.3)\n",
            "Collecting SimpleITK>=2.2.1 (from nnunetv2==2.4.2)\n",
            "  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.7/52.7 MB 26.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.4.2) (2.0.3)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.4.2) (0.20.3)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.4.2) (2024.5.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.4.2) (2.31.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.4.2) (4.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.4.2) (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from nnunetv2==2.4.2) (0.13.1)\n",
            "Collecting imagecodecs (from nnunetv2==2.4.2)\n",
            "  Downloading imagecodecs-2024.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39.6/39.6 MB 42.9 MB/s eta 0:00:00\n",
            "Collecting yacs (from nnunetv2==2.4.2)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Collecting connected-components-3d (from acvl-utils<0.3,>=0.2->nnunetv2==2.4.2)\n",
            "  Downloading connected_components_3d-3.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 65.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2==2.4.2) (9.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2==2.4.2) (0.18.3)\n",
            "Collecting unittest2 (from batchgenerators>=0.25->nnunetv2==2.4.2)\n",
            "  Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.4/96.4 kB 12.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2==2.4.2) (3.5.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2==2.4.2) (3.3)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2==2.4.2) (2.31.6)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2==2.4.2) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2==2.4.2) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2==2.4.2) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2==2.4.2) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2==2.4.2) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2==2.4.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2==2.4.2) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.1.2->nnunetv2==2.4.2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.1.2->nnunetv2==2.4.2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.1.2->nnunetv2==2.4.2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.1.2->nnunetv2==2.4.2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.1.2->nnunetv2==2.4.2)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.1.2->nnunetv2==2.4.2)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.1.2->nnunetv2==2.4.2)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.1.2->nnunetv2==2.4.2)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.1.2->nnunetv2==2.4.2)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.1.2->nnunetv2==2.4.2)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.1.2->nnunetv2==2.4.2)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2==2.4.2) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.2->nnunetv2==2.4.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/21.3 MB 73.8 MB/s eta 0:00:00\n",
            "Collecting pydicom>=2.2.0 (from dicom2nifti->nnunetv2==2.4.2)\n",
            "  Downloading pydicom-2.4.4-py3-none-any.whl (1.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 90.2 MB/s eta 0:00:00\n",
            "Collecting python-gdcm (from dicom2nifti->nnunetv2==2.4.2)\n",
            "  Downloading python_gdcm-3.0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 12.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2==2.4.2) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2==2.4.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2==2.4.2) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2==2.4.2) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2==2.4.2) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2==2.4.2) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel->nnunetv2==2.4.2) (67.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nnunetv2==2.4.2) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nnunetv2==2.4.2) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2==2.4.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2==2.4.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2==2.4.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2==2.4.2) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nnunetv2==2.4.2) (1.4.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs->nnunetv2==2.4.2) (6.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->nnunetv2==2.4.2) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.2->nnunetv2==2.4.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.2->nnunetv2==2.4.2) (1.3.0)\n",
            "Collecting argparse (from unittest2->batchgenerators>=0.25->nnunetv2==2.4.2)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting traceback2 (from unittest2->batchgenerators>=0.25->nnunetv2==2.4.2)\n",
            "  Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting linecache2 (from traceback2->unittest2->batchgenerators>=0.25->nnunetv2==2.4.2)\n",
            "  Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: nnunetv2, acvl-utils, batchgenerators, dynamic-network-architectures\n",
            "  Building editable for nnunetv2 (pyproject.toml): started\n",
            "  Building editable for nnunetv2 (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for nnunetv2: filename=nnunetv2-2.4.2-0.editable-py3-none-any.whl size=16658 sha256=440b767ab753c9906a2e14699d68b26878bbcf128e66b7aaf6d4ae6b93ccdea2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c3n9s8dl/wheels/1a/72/d7/bbc13a2ce05a914e946a31943e5dbc2aa667ff2e13fb1d02d4\n",
            "  Building wheel for acvl-utils (setup.py): started\n",
            "  Building wheel for acvl-utils (setup.py): finished with status 'done'\n",
            "  Created wheel for acvl-utils: filename=acvl_utils-0.2-py3-none-any.whl size=22439 sha256=becd88617c5041c69a16d274868836508ce0c05ecba79a348689e029a21ceecc\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/f0/84/52e8897591e66339bd2796681b9540b6c5e453c1461fa92a9e\n",
            "  Building wheel for batchgenerators (setup.py): started\n",
            "  Building wheel for batchgenerators (setup.py): finished with status 'done'\n",
            "  Created wheel for batchgenerators: filename=batchgenerators-0.25-py3-none-any.whl size=89007 sha256=67d56804e3c5e7524739bfcfafdd443b4c4bebb735898296c389c3627caf4b50\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/b0/1b/40912fb58eb167b86cbc444ddb2e6ba382b248215295f932e2\n",
            "  Building wheel for dynamic-network-architectures (setup.py): started\n",
            "  Building wheel for dynamic-network-architectures (setup.py): finished with status 'done'\n",
            "  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.3.1-py3-none-any.whl size=30049 sha256=1b551dee6225f5019f2a6bcae6a2580a5697d0d383a14f81943fb41d3be0d222\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/1b/13/a6419c8dbf998b9343710355ec3edc5c8e24d9b7b22eec95fb\n",
            "Successfully built nnunetv2 acvl-utils batchgenerators dynamic-network-architectures\n",
            "Installing collected packages: SimpleITK, linecache2, argparse, yacs, traceback2, python-gdcm, pydicom, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, imagecodecs, connected-components-3d, unittest2, nvidia-cusparse-cu12, nvidia-cudnn-cu12, dicom2nifti, nvidia-cusolver-cu12, batchgenerators, dynamic-network-architectures, acvl-utils, nnunetv2\n",
            "Successfully installed SimpleITK-2.3.1 acvl-utils-0.2 argparse-1.4.0 batchgenerators-0.25 connected-components-3d-3.16.0 dicom2nifti-2.4.11 dynamic-network-architectures-0.3.1 imagecodecs-2024.1.1 linecache2-1.0.0 nnunetv2-2.4.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pydicom-2.4.4 python-gdcm-3.0.24.1 traceback2-1.4.0 unittest2-1.1.0 yacs-0.1.8\n",
            "Collecting git+https://github.com/FabianIsensee/hiddenlayer.git\n",
            "  Cloning https://github.com/FabianIsensee/hiddenlayer.git to /tmp/pip-req-build-efdj3j_f\n",
            "  Resolved https://github.com/FabianIsensee/hiddenlayer.git to commit b7263b6dc4569da1b6dea5964e1eac78fa32fa77\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Building wheels for collected packages: hiddenlayer\n",
            "  Building wheel for hiddenlayer (setup.py): started\n",
            "  Building wheel for hiddenlayer (setup.py): finished with status 'done'\n",
            "  Created wheel for hiddenlayer: filename=hiddenlayer-0.2-py3-none-any.whl size=20003 sha256=892ff99b2cda42996bb6c29fe66c0d3ac5df8d8448bfb1e35865530756b80541\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pken6cxd/wheels/55/0e/e3/fdf2f92789305c0e320e0ab01f27fd4b757b1bb01c07d532c4\n",
            "Successfully built hiddenlayer\n",
            "Installing collected packages: hiddenlayer\n",
            "Successfully installed hiddenlayer-0.2\n",
            "Collecting ants\n",
            "  Downloading ants-0.0.7.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting Django>=1.10.2 (from ants)\n",
            "  Downloading Django-5.0.6-py3-none-any.whl (8.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.2/8.2 MB 79.8 MB/s eta 0:00:00\n",
            "Collecting gevent>=1.1.1 (from ants)\n",
            "  Downloading gevent-24.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (6.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 90.6 MB/s eta 0:00:00\n",
            "Collecting asgiref<4,>=3.7.0 (from Django>=1.10.2->ants)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from Django>=1.10.2->ants) (0.5.0)\n",
            "Collecting zope.event (from gevent>=1.1.1->ants)\n",
            "  Downloading zope.event-5.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting zope.interface (from gevent>=1.1.1->ants)\n",
            "  Downloading zope.interface-6.4.post2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.8/247.8 kB 32.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gevent>=1.1.1->ants) (3.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from asgiref<4,>=3.7.0->Django>=1.10.2->ants) (4.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.event->gevent>=1.1.1->ants) (67.7.2)\n",
            "Building wheels for collected packages: ants\n",
            "  Building wheel for ants (setup.py): started\n",
            "  Building wheel for ants (setup.py): finished with status 'done'\n",
            "  Created wheel for ants: filename=ants-0.0.7-py3-none-any.whl size=14521 sha256=e15ee20db5db6b2be09d8d94944b488ec672eca9b0a9fcce3f80378a05477262\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/36/ad/719ec3c226c5547a8ed5a668a5fc95cf01919db0453dc220ca\n",
            "Successfully built ants\n",
            "Installing collected packages: zope.interface, zope.event, asgiref, gevent, Django, ants\n",
            "Successfully installed Django-5.0.6 ants-0.0.7 asgiref-3.8.1 gevent-24.2.1 zope.event-5.0 zope.interface-6.4.post2\n",
            "Collecting antspyx\n",
            "  Downloading antspyx-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345.9 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 345.9/345.9 MB 3.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from antspyx) (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from antspyx) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from antspyx) (1.11.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from antspyx) (0.19.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from antspyx) (1.2.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from antspyx) (0.14.2)\n",
            "Requirement already satisfied: webcolors in /usr/local/lib/python3.10/dist-packages (from antspyx) (1.13)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from antspyx) (3.7.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from antspyx) (6.0.1)\n",
            "Collecting chart-studio (from antspyx)\n",
            "  Downloading chart_studio-1.1.0-py3-none-any.whl (64 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.4/64.4 kB 9.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from antspyx) (9.4.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from antspyx) (4.0.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from chart-studio->antspyx) (5.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from chart-studio->antspyx) (2.31.0)\n",
            "Collecting retrying>=1.3.3 (from chart-studio->antspyx)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from chart-studio->antspyx) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->antspyx) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->antspyx) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->antspyx) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->antspyx) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->antspyx) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->antspyx) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->antspyx) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel->antspyx) (67.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->antspyx) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->antspyx) (2024.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->antspyx) (3.3)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->antspyx) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->antspyx) (2024.5.10)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->antspyx) (1.6.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->antspyx) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->antspyx) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->antspyx) (0.5.6)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->chart-studio->antspyx) (8.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->chart-studio->antspyx) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->chart-studio->antspyx) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->chart-studio->antspyx) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->chart-studio->antspyx) (2024.2.2)\n",
            "Installing collected packages: retrying, chart-studio, antspyx\n",
            "Successfully installed antspyx-0.4.2 chart-studio-1.1.0 retrying-1.3.4\n",
            "Collecting intensity_normalization\n",
            "  Downloading intensity_normalization-2.2.4-py3-none-any.whl (50 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.8/50.8 kB 1.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: SimpleITK<3,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from intensity_normalization) (2.3.1)\n",
            "Collecting nibabel<4,>=3 (from intensity_normalization)\n",
            "  Downloading nibabel-3.2.2-py3-none-any.whl (3.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 54.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy<2,>=1.22 in /usr/local/lib/python3.10/dist-packages (from intensity_normalization) (1.25.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from intensity_normalization) (9.4.0)\n",
            "Requirement already satisfied: pydicom<3,>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from intensity_normalization) (2.4.4)\n",
            "Collecting pymedio<1,>=0.2.8 (from intensity_normalization)\n",
            "  Downloading pymedio-0.2.14-py3-none-any.whl (26 kB)\n",
            "Collecting scikit-fuzzy<1,>=0.4.2 (from intensity_normalization)\n",
            "  Downloading scikit-fuzzy-0.4.2.tar.gz (993 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 994.0/994.0 kB 65.3 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: scikit-image<1,>=0.17 in /usr/local/lib/python3.10/dist-packages (from intensity_normalization) (0.19.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=0.24 in /usr/local/lib/python3.10/dist-packages (from intensity_normalization) (1.2.2)\n",
            "Requirement already satisfied: scipy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from intensity_normalization) (1.11.4)\n",
            "Requirement already satisfied: statsmodels<1,>=0.12 in /usr/local/lib/python3.10/dist-packages (from intensity_normalization) (0.14.2)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from nibabel<4,>=3->intensity_normalization) (24.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel<4,>=3->intensity_normalization) (67.7.2)\n",
            "Requirement already satisfied: networkx>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from scikit-fuzzy<1,>=0.4.2->intensity_normalization) (3.3)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<1,>=0.17->intensity_normalization) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image<1,>=0.17->intensity_normalization) (2024.5.10)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<1,>=0.17->intensity_normalization) (1.6.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2,>=0.24->intensity_normalization) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2,>=0.24->intensity_normalization) (3.5.0)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels<1,>=0.12->intensity_normalization) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels<1,>=0.12->intensity_normalization) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels<1,>=0.12->intensity_normalization) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels<1,>=0.12->intensity_normalization) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels<1,>=0.12->intensity_normalization) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels<1,>=0.12->intensity_normalization) (1.16.0)\n",
            "Building wheels for collected packages: scikit-fuzzy\n",
            "  Building wheel for scikit-fuzzy (setup.py): started\n",
            "  Building wheel for scikit-fuzzy (setup.py): finished with status 'done'\n",
            "  Created wheel for scikit-fuzzy: filename=scikit_fuzzy-0.4.2-py3-none-any.whl size=894078 sha256=249061ee1dad00636826b94a0c43f5f397950eb5b9f028419fc3422f6f79b217\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/86/1b/dfd97134a2c8313e519bcebd95d3fedc7be7944db022094bc8\n",
            "Successfully built scikit-fuzzy\n",
            "Installing collected packages: pymedio, nibabel, scikit-fuzzy, intensity_normalization\n",
            "  Attempting uninstall: nibabel\n",
            "    Found existing installation: nibabel 4.0.2\n",
            "    Uninstalling nibabel-4.0.2:\n",
            "      Successfully uninstalled nibabel-4.0.2\n",
            "Successfully installed intensity_normalization-2.2.4 nibabel-3.2.2 pymedio-0.2.14 scikit-fuzzy-0.4.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/FabianIsensee/hiddenlayer.git /tmp/pip-req-build-efdj3j_f\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "# Navega al directorio donde se desea clonar el repositorio nnUNet.\n",
        "cd /content/drive/MyDrive/nnUNetFrame\n",
        "\n",
        "# Clona el repositorio nnUNet desde GitHub.\n",
        "# Descomentar la siguiente línea si es la primera vez que se clona el repositorio.\n",
        "# git clone https://github.com/MIC-DKFZ/nnUNet.git\n",
        "\n",
        "# Cambia al directorio del repositorio nnUNet.\n",
        "cd nnUNet\n",
        "\n",
        "# Instala nnUNet en modo editable.\n",
        "pip install -e .\n",
        "\n",
        "# Vuelve al directorio nnUNetFrame.\n",
        "cd ..\n",
        "\n",
        "# Actualiza el paquete hiddenlayer desde el repositorio de GitHub de FabianIsensee.\n",
        "pip install --upgrade git+https://github.com/FabianIsensee/hiddenlayer.git\n",
        "\n",
        "# Instala el paquete ants.\n",
        "pip install ants\n",
        "\n",
        "# Instala el paquete antspyx.\n",
        "pip install antspyx\n",
        "\n",
        "# Instala el paquete intensity_normalization.\n",
        "pip install intensity_normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxE_4KAF0ySE",
        "outputId": "e61daa14-3c03-4ab0-d24b-d1a8f8669f8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch version：\n",
            "2.3.0+cu121\n",
            "CUDA Version: \n",
            "12.1\n",
            "cuDNN version is :\n",
            "8902\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import SimpleITK as sitk\n",
        "import ants\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import nibabel as nib\n",
        "import glob\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "from intensity_normalization.typing import Modality, TissueType\n",
        "from intensity_normalization.normalize.nyul import NyulNormalize\n",
        "\n",
        "# Imprimir la versión de PyTorch\n",
        "print(\"Pytorch version：\")\n",
        "print(torch.__version__)\n",
        "\n",
        "# Imprimir la versión de CUDA\n",
        "print(\"CUDA Version: \")\n",
        "print(torch.version.cuda)\n",
        "\n",
        "# Imprimir la versión de cuDNN\n",
        "print(\"cuDNN version is :\")\n",
        "print(torch.backends.cudnn.version())\n",
        "\n",
        "\n",
        "def prepare_dataset(origin_path: str, template_type: str = \"FLAIR_template.nii.gz\") -> None:\n",
        "    \"\"\"\n",
        "    Prepares the dataset by creating necessary directories and calling preprocessing functions.\n",
        "\n",
        "    Parameters:\n",
        "    - origin_path (str): The path to the original dataset.\n",
        "    - template_type (str): The name of the template file.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "\n",
        "    template = os.path.join(os.environ[\"base_path\"], template_type)\n",
        "    dest_path = os.path.join(os.environ[\"nnUNet_raw\"], \"Dataset001_Stroke\")\n",
        "    path_images = os.path.join(dest_path, \"imagesTr\")\n",
        "    path_labels = os.path.join(dest_path, \"labelsTr\")\n",
        "\n",
        "    # Crear directorios si no existen\n",
        "    if not os.path.exists(path_images):\n",
        "        os.makedirs(path_images, exist_ok=True)\n",
        "        os.mkdir(path_labels)\n",
        "\n",
        "    diccionario = emparejar(origin_path, dest_path)\n",
        "    preprocess(diccionario, template)\n",
        "\n",
        "\n",
        "def buscar_por_patron(lista: list, caso: str) -> str:\n",
        "    \"\"\"\n",
        "    Searches for a file matching a specific pattern in a list.\n",
        "\n",
        "    Parameters:\n",
        "    - lista (list): List of filenames to search in.\n",
        "    - caso (str): Case identifier to search for.\n",
        "\n",
        "    Returns:\n",
        "    - str: The filename that matches the pattern, or None if not found.\n",
        "    \"\"\"\n",
        "\n",
        "    # En este caso seguimos la estructura de los datos de ISLES\n",
        "    string = f'sub-strokecase{caso}_ses-0001_msk'\n",
        "    pattern = re.compile(string)\n",
        "\n",
        "    for elemento in lista:\n",
        "        if pattern.search(elemento):\n",
        "            return elemento\n",
        "    return None\n",
        "\n",
        "\n",
        "def emparejar(base_path: str, dest_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Pairs FLAIR images with their corresponding mask files.\n",
        "\n",
        "    Parameters:\n",
        "    - base_path (str): The path to the base directory containing the FLAIR and mask files.\n",
        "    - dest_path (str): The destination path where the paired files will be stored.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary containing the paired files.\n",
        "    \"\"\"\n",
        "    flair_files = glob.glob(os.path.join(base_path, \"FLAIR_reg\", \"*\"))\n",
        "    mask_files = glob.glob(os.path.join(base_path, \"LABELS_reg\", \"*\"))\n",
        "\n",
        "    resultado = {}\n",
        "    pattern = re.compile(r'sub-strokecase(\\d+)_ses-\\d+_(FLAIR|msk)\\.nii\\.gz')\n",
        "\n",
        "    for image_orig in flair_files:\n",
        "        encont = pattern.search(image_orig)\n",
        "        caso = encont.group(1)\n",
        "        label_orig = buscar_por_patron(mask_files, caso)\n",
        "\n",
        "        resultado[caso] = {\n",
        "            'image': {\n",
        "                'origin': image_orig,\n",
        "                'dest': os.path.join(dest_path, 'imagesTr', f'ISLES_{caso}_0000.nii.gz')\n",
        "            },\n",
        "            'label': {\n",
        "                'origin': label_orig,\n",
        "                'dest': os.path.join(dest_path, 'labelsTr', f'ISLES_{caso}.nii.gz')\n",
        "            }\n",
        "        }\n",
        "\n",
        "    return resultado\n",
        "\n",
        "\n",
        "def convert_labels_to_nnUNet(image: sitk.Image) -> sitk.Image:\n",
        "    \"\"\"\n",
        "    Converts label images to the nnUNet format.\n",
        "\n",
        "    Parameters:\n",
        "    - image (sitk.Image): The input label image.\n",
        "\n",
        "    Returns:\n",
        "    - sitk.Image: The converted label image.\n",
        "    \"\"\"\n",
        "    img_npy = sitk.GetArrayFromImage(image)\n",
        "    seg_new = np.zeros_like(img_npy)\n",
        "    seg_new[img_npy >= 1] = 1\n",
        "    seg_new[img_npy < 1] = 0\n",
        "    img_corr = sitk.GetImageFromArray(seg_new)\n",
        "    img_corr.CopyInformation(image)\n",
        "    return img_corr\n",
        "\n",
        "def bias_field_correct(raw_img_sitk: sitk.Image) -> sitk.Image:\n",
        "    \"\"\"\n",
        "    Performs bias field correction on an input image.\n",
        "\n",
        "    Parameters:\n",
        "    - raw_img_sitk (sitk.Image): The input image to be corrected.\n",
        "\n",
        "    Returns:\n",
        "    - sitk.Image: The bias field corrected image.\n",
        "    \"\"\"\n",
        "    raw_img_sitk_arr = sitk.GetArrayFromImage(raw_img_sitk)\n",
        "    transformed = sitk.RescaleIntensity(raw_img_sitk, 0, 255)\n",
        "    transformed = sitk.LiThreshold(transformed, 0, 1)\n",
        "    head_mask = transformed\n",
        "    shrinkFactor = 4\n",
        "    inputImage = raw_img_sitk\n",
        "\n",
        "    inputImage = sitk.Shrink(raw_img_sitk, [shrinkFactor] * inputImage.GetDimension())\n",
        "    maskImage = sitk.Shrink(head_mask, [shrinkFactor] * inputImage.GetDimension())\n",
        "    bias_corrector = sitk.N4BiasFieldCorrectionImageFilter()\n",
        "    corrected = bias_corrector.Execute(inputImage, maskImage)\n",
        "    log_bias_field = bias_corrector.GetLogBiasFieldAsImage(raw_img_sitk)\n",
        "    corrected_image_full_resolution = raw_img_sitk / sitk.Exp(log_bias_field)\n",
        "\n",
        "    return corrected_image_full_resolution\n",
        "\n",
        "\n",
        "def fit_normalizer(standard_histogram_path: str) -> NyulNormalize:\n",
        "    \"\"\"\n",
        "    Fits a normalizer using the Nyul normalization method.\n",
        "\n",
        "    Parameters:\n",
        "    - standard_histogram_path (str): Path to save or load the standard histogram.\n",
        "\n",
        "    Returns:\n",
        "    - NyulNormalize: The fitted normalizer.\n",
        "    \"\"\"\n",
        "    normalizer = NyulNormalize()\n",
        "    if os.path.exists(standard_histogram_path):\n",
        "        normalizer.load_standard_histogram(standard_histogram_path)\n",
        "    else:\n",
        "        images_all = glob.glob(os.path.join(os.environ[\"base_path\"], \"FLAIR\", \"*\"))\n",
        "        images = [nib.load(path).get_fdata() for path in images_all]\n",
        "        normalizer.fit(images)\n",
        "        del images\n",
        "        normalizer.save_standard_histogram(standard_histogram_path)\n",
        "    return normalizer\n",
        "\n",
        "def IntensityNormalization(image: sitk.Image, model: NyulNormalize) -> sitk.Image:\n",
        "    \"\"\"\n",
        "    Normalizes the intensity of an image using a given model.\n",
        "\n",
        "    Parameters:\n",
        "    - image (sitk.Image): The input image.\n",
        "    - model (NyulNormalize): The normalization model.\n",
        "\n",
        "    Returns:\n",
        "    - sitk.Image: The intensity-normalized image.\n",
        "    \"\"\"\n",
        "    image_arr = sitk.GetArrayFromImage(image)\n",
        "    new_image = model(image_arr)\n",
        "    new_image = sitk.GetImageFromArray(new_image)\n",
        "    new_image.CopyInformation(image)\n",
        "    return new_image\n",
        "\n",
        "\n",
        "\n",
        "def process_case(args: tuple) -> None:\n",
        "    \"\"\"\n",
        "    Processes a single case by applying bias correction, intensity normalization, and label conversion.\n",
        "\n",
        "    Parameters:\n",
        "    - args (tuple): A tuple containing case identifier, data dictionary, and template path.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    caso, datos, template_path = args\n",
        "\n",
        "    # Cargar las imágenes\n",
        "    image_raw = sitk.ReadImage(datos[caso][\"image\"][\"origin\"], sitk.sitkFloat32)\n",
        "    label_raw = sitk.ReadImage(datos[caso][\"label\"][\"origin\"], sitk.sitkFloat32)\n",
        "    template_read = ants.image_read(template_path, reorient='IAL')\n",
        "\n",
        "    # Paso 1: Corrección de campo de sesgo\n",
        "    image_bias = bias_field_correct(image_raw)\n",
        "\n",
        "    # Paso 2: Normalización de intensidades\n",
        "    norm_histo = os.path.join(os.environ[\"base_path\"], \"standard_histogram.npy\")\n",
        "    normalizer = fit_normalizer(norm_histo)\n",
        "    image_trans = IntensityNormalization(image_bias, normalizer)\n",
        "\n",
        "    # Paso 3: Corrección de etiquetas\n",
        "    label_corr = convert_labels_to_nnUNet(label_raw)\n",
        "\n",
        "    # Guardar las imágenes procesadas\n",
        "    sitk.WriteImage(label_corr, datos[caso][\"label\"][\"dest\"])\n",
        "    sitk.WriteImage(image_trans, datos[caso][\"image\"][\"dest\"])\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(datos_dic: dict, template_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Preprocesses the dataset using multiprocessing.\n",
        "\n",
        "    Parameters:\n",
        "    - datos_dic (dict): Dictionary containing data information.\n",
        "    - template_path (str): Path to the template file.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    num_cores = multiprocessing.cpu_count()  # Número de núcleos disponibles\n",
        "\n",
        "    # Crear un pool de procesos\n",
        "    with multiprocessing.Pool(num_cores) as pool:\n",
        "        args_list = [(caso, datos_dic, template_path) for caso in datos_dic.keys()]\n",
        "\n",
        "        # Configurar la barra de progreso\n",
        "        with tqdm(total=len(args_list), desc=\"Processing cases\") as pbar:\n",
        "            for result in pool.imap_unordered(process_case, args_list):\n",
        "                pbar.update()  # Actualizar la barra de progreso por cada tarea completada\n",
        "\n",
        "\n",
        "# Por comodidad, las imágenes fueron registradas previamente en el dataset\n",
        "\n",
        "def register_image_mask(template_read: ants.ANTsImage, image_read: ants.ANTsImage, mask_read: ants.ANTsImage) -> tuple:\n",
        "    \"\"\"\n",
        "    Registers the image and mask to the template.\n",
        "\n",
        "    Parameters:\n",
        "    - template_read (ants.ANTsImage): The template image.\n",
        "    - image_read (ants.ANTsImage): The image to be registered.\n",
        "    - mask_read (ants.ANTsImage): The mask to be registered.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Registered mask and image.\n",
        "    \"\"\"\n",
        "    transformation = ants.registration(\n",
        "        fixed=template_read,\n",
        "        moving=image_read,\n",
        "        type_of_transform='Rigid',\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    registered_img = transformation['warpedmovout']\n",
        "\n",
        "    registered_mask_img = ants.apply_transforms(\n",
        "        moving=mask_read,\n",
        "        interpolator=\"bSpline\",\n",
        "        fixed=transformation['warpedmovout'],\n",
        "        transformlist=transformation['fwdtransforms'],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    return registered_mask_img, registered_img\n",
        "\n",
        "def itk_2_ants(image: sitk.Image) -> ants.ANTsImage:\n",
        "    \"\"\"\n",
        "    Converts an ITK image to an ANTs image.\n",
        "\n",
        "    Parameters:\n",
        "    - image (sitk.Image): The ITK image to convert.\n",
        "\n",
        "    Returns:\n",
        "    - ants.ANTsImage: The converted ANTs image.\n",
        "    \"\"\"\n",
        "    image_ants = ants.from_numpy(sitk.GetArrayFromImage(image).T,\n",
        "                                 origin=image.GetOrigin(),\n",
        "                                 spacing=image.GetSpacing(),\n",
        "                                 direction=np.array(image.GetDirection()).reshape(3, 3))\n",
        "    return image_ants\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preparación del dataset**\n",
        "\n",
        "Para la preparación del dataset se realizaron los siguientes pasos:\n",
        "\n",
        "1.- **Registro de imágenes (normalización espacial)**: El registro de imágenes consiste en transportar todas las imágenes a un espacio común que permita realizar el análisis. Por comodidad a la hora de la instalación de diferentes softwares, el proceso fue realizado en un ordenador local. Dentro del registro, se realizó un registro al espacio MNI (Montreal Neurological Institute) que es un estándar dentro de la neuroimagen. Para el registro se realizó un registro rígido con 6 grados de libertad de las imágenes FLAIR, y posteriormente se aplicaron las transformaciones a las correspondientes máscaras. El registro fue realizado mediente el comando FLIRT contenido en el paquete FSL y se realizó una interpolación nearestNeighbour para las máscaras. Para la evalución de la calidad de los registros se empleó la herramienta slicesdir.\n",
        "\n",
        "A partir de este paso de registro, las imágenes fueron subidas a Google Drive y están accesibles en este link\n",
        "\n",
        "2.- **Corrección del sesgo de campo (bias field correction)**: debido a las inhomogeneidades del campo magnético producido por la resonancia magnética es necesario realizar un paso de normalización que corrige estas inhomogeneidades que afectan a la intensidad de la señal. Esto se realizó mediante la corrección N4 del paquete SimpleITK\n",
        "\n",
        "3.- **Normalización de intensidad**: para la normalización de la intensidad de señal se empleó el método de Nyul. El método de Nyul se basa en la idea de ajustar las intensidades de las imágenes de tal manera que las distribuciones de intensidad de diferentes imágenes se alineen con una distribución estándar predefinida. Esto se hace en varias etapas:\n",
        "\n",
        "- Selección de una plantilla: Se selecciona una imagen o un conjunto de imágenes que sirven como plantilla o estándar para la normalización.\n",
        "\n",
        "- Cálculo del histograma estándar: Se calcula un histograma de intensidades para la imagen plantilla. Este histograma sirve como la distribución de referencia a la que se ajustarán las demás imágenes. Se realizó dicha normalización en base a todas las imágenes del dataset\n",
        "\n",
        "- Transformación de las intensidades: Se determinan las transformaciones necesarias para ajustar las intensidades de una imagen dada a la distribución de intensidades de la plantilla. Esto implica mapear los percentiles de intensidad de la imagen original a los percentiles correspondientes en el histograma estándar.\n",
        "\n",
        "- Aplicación de la transformación: Las intensidades de la imagen original se transforman de acuerdo con la función de mapeo calculada, resultando en una imagen con una distribución de intensidades alineada con la plantilla.\n",
        "\n",
        "Las ventajas de la normalización de la intensidad de Nyul es que permite reducir la variabilidad intersujeto y así mejorar los resultados de los datos.\n",
        "\n",
        "4.- **Comprobación de la integridad de las etiquetas de las máscaras**: Finalmente nos aseguramos que cumple correctamente el formato adecuado para nnUNET.\n",
        "\n",
        "El código anterior también asegura que se mantiene de forma íntegra la estructura de los datos necesarios para el procesamiento por nnUNet\n",
        "\n",
        "```\n",
        "nnUNet_raw/Dataset001_NAME1\n",
        "├── dataset.json\n",
        "├── imagesTr\n",
        "│   ├── ...\n",
        "├── imagesTs\n",
        "│   ├── ...\n",
        "└── labelsTr\n",
        "    ├── ...\n",
        "nnUNet_raw/Dataset002_NAME2\n",
        "├── dataset.json\n",
        "├── imagesTr\n",
        "│   ├── ...\n",
        "├── imagesTs\n",
        "│   ├── ...\n",
        "└── labelsTr\n",
        "    ├── ...\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "NTo-k7GPVJsq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeuU-5UVVnqs",
        "outputId": "e2c41f13-4a72-48c3-8da2-468c2c0c09f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing cases: 100%|██████████| 250/250 [05:57<00:00,  1.43s/it]\n"
          ]
        }
      ],
      "source": [
        "# Preparamos el dataset requerido\n",
        "\n",
        "prepare_dataset(os.environ[\"base_path\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación de los planes de nnUNet\n",
        "\n",
        "Para ejecutar el entrenamiento es necesario que se genere un archivo .json que permita interpretar qué planes debe de ejecutar el algoritmo, así como algunas estadísticas."
      ],
      "metadata": {
        "id": "5C46Ryd5rKKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umdnNHBY0ySG"
      },
      "outputs": [],
      "source": [
        "# Cambiar el directorio de trabajo al directorio nnUNet dentro del directorio MyDrive\n",
        "os.chdir(os.path.join(nnFrame, \"nnUNet\"))\n",
        "\n",
        "# Importaciones\n",
        "from nnunetv2.dataset_conversion.generate_dataset_json import generate_dataset_json\n",
        "from batchgenerators.utilities.file_and_folder_operations import *\n",
        "from typing import Tuple\n",
        "\n",
        "# Definir el directorio base de salida usando una variable de entorno\n",
        "out_base = join(os.environ['nnUNet_raw'], foldername)\n",
        "\n",
        "# Definir las rutas para las carpetas de imágenes de entrenamiento y etiquetas\n",
        "imagestr = join(out_base, \"imagesTr\")\n",
        "labelstr = join(out_base, \"labelsTr\")\n",
        "\n",
        "# Obtener una lista de todos los casos de entrenamiento (archivos) en la carpeta de imágenes de entrenamiento\n",
        "case_ids = glob.glob(join(imagestr, \"*\"))\n",
        "\n",
        "# Generar el archivo JSON del dataset con la función generate_dataset_json\n",
        "generate_dataset_json(\n",
        "    out_base,                           # Ruta base de salida\n",
        "    channel_names={\"0\": 'noNorm'},      # Nombre del canal (en este caso, sin normalización)\n",
        "    labels={                            # Etiquetas del dataset con sus respectivos valores\n",
        "        'background': 0,\n",
        "        'infarct': 1\n",
        "    },\n",
        "    tensorImageSize=\"3D\",               # Indicar que las imágenes son 3D\n",
        "    num_training_cases=len(case_ids),   # Número de casos de entrenamiento\n",
        "    file_ending='.nii.gz',              # Extensión de los archivos de imagen\n",
        "    dataset_release='1.0'               # Versión del dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comprobando la integridad del dataset\n",
        "\n",
        "Tras esto, el paquete nnUNet permite comprobar la integridad del dataset propuesto"
      ],
      "metadata": {
        "id": "jNu_MK-bv_m7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7MjcabCQ6eJ",
        "outputId": "ed0c9daf-6ab7-4b4c-dd2f-ae6ad261e6ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "# Cambia el directorio de trabajo al directorio nnUNet_raw dentro del directorio dataset\n",
        "cd $nnUNet_raw\n",
        "\n",
        "# Ejecuta el comando nnUNetv2_plan_and_preprocess con varios parámetros:\n",
        "# -d 1: Especifica el número del dataset (1 en este caso).\n",
        "# -npfp 12: Define el número de procesos paralelos para la planificación (12 en este caso).\n",
        "# --verify_dataset_integrity: Verifica la integridad del dataset antes de la planificación y el preprocesamiento.\n",
        "# -pl nnUNetPlannerResEncL: Especifica el planificador a utilizar (nnUNetPlannerResEncL en este caso).\n",
        "\n",
        "nnUNetv2_plan_and_preprocess -d 1 -npfp 12 --verify_dataset_integrity -pl nnUNetPlannerResEncL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del modelo\n",
        "\n",
        "Esto permitirá entrenar el modelo solicitado. Podemos entrenarlo 5 veces, seleccionando el tipo de configuración solicitada. Mediante el flag --np se guardan las predicciones para posteriormente poder hacer un ensambling"
      ],
      "metadata": {
        "id": "TpLfuR0fwIWk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBTGP68Zypzk"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "## Entrenamos durante 5 folds el algoritmo\n",
        "\n",
        "# Entrena el modelo nnUNet para el dataset 1 (Stroke) utilizando\n",
        "# la configuración 3d_fullres y el planificador nnUNetResEncUNetLPlans.\n",
        "# El entrenamiento se ejecuta en cinco diferentes pliegues (folds)\n",
        "# para realizar validación cruzada.\n",
        "# La opción --npz se utiliza para guardar las predicciones como archivos .npz.\n",
        "\n",
        "\n",
        "nnUNetv2_train 1 3d_fullres 0 -p nnUNetResEncUNetLPlans --npz &&\n",
        "nnUNetv2_train 1 3d_fullres 1 -p nnUNetResEncUNetLPlans --npz &&\n",
        "nnUNetv2_train 1 3d_fullres 2 -p nnUNetResEncUNetLPlans --npz &&\n",
        "nnUNetv2_train 1 3d_fullres 3 -p nnUNetResEncUNetLPlans --npz &&\n",
        "nnUNetv2_train 1 3d_fullres 4 -p nnUNetResEncUNetLPlans --npz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Búsqueda de la mejor configuración\n",
        "\n",
        "En el siguiente comando permite determinar cuál es la mejor configuración del modelo"
      ],
      "metadata": {
        "id": "U2Fgv_81wd59"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHtYHUOmfL34",
        "outputId": "9c1fb25b-6934-4327-92b1-74c12d57d03e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "***All results:***\n",
            "nnUNetTrainer__nnUNetResEncUNetLPlans__2d: 0.4146338413159022\n",
            "\n",
            "*Best*: nnUNetTrainer__nnUNetResEncUNetLPlans__2d: 0.4146338413159022\n",
            "\n",
            "***Determining postprocessing for best model/ensemble***\n",
            "Removing all but the largest foreground region did not improve results!\n",
            "\n",
            "***Run inference like this:***\n",
            "\n",
            "nnUNetv2_predict -d Dataset001_Stroke -i INPUT_FOLDER -o OUTPUT_FOLDER -f  0 1 2 3 4 -tr nnUNetTrainer -c 2d -p nnUNetResEncUNetLPlans\n",
            "\n",
            "***Once inference is completed, run postprocessing like this:***\n",
            "\n",
            "nnUNetv2_apply_postprocessing -i OUTPUT_FOLDER -o OUTPUT_FOLDER_PP -pp_pkl_file /content/drive/MyDrive/nnUNetFrame/dataset/nnUNet_trained_models/Dataset001_Stroke/nnUNetTrainer__nnUNetResEncUNetLPlans__2d/crossval_results_folds_0_1_2_3_4/postprocessing.pkl -np 8 -plans_json /content/drive/MyDrive/nnUNetFrame/dataset/nnUNet_trained_models/Dataset001_Stroke/nnUNetTrainer__nnUNetResEncUNetLPlans__2d/crossval_results_folds_0_1_2_3_4/plans.json\n"
          ]
        }
      ],
      "source": [
        "# Ejecuta el comando nnUNetv2_find_best_configuration para encontrar\n",
        "# la mejor configuración de entrenamiento para el dataset 1.\n",
        "# La opción -c 3d_fullres especifica que se debe considerar la configuración\n",
        "# en 3D.\n",
        "# La opción -p nnUNetResEncUNetLPlans indica que se debe utilizar\n",
        "# el planificador nnUNetResEncUNetLPlans.\n",
        "\n",
        "!nnUNetv2_find_best_configuration 1 -c 3d_fullres -p nnUNetResEncUNetLPlans"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicción de los resultados y postprocesado\n",
        "\n",
        "Para finalmente validar los resultados encontrados en el dataset en cuestión realizaremos prueba con los datos proporcionados. Estos se almacenarán en una carpeta que se llamará test, e input"
      ],
      "metadata": {
        "id": "zjDQu0XRyWbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# Crear las carpetas de entrada y salida si no existen usando variables de entorno\n",
        "mkdir -p $work_nnUNet/test/input\n",
        "mkdir -p $work_nnUNet/test/output"
      ],
      "metadata": {
        "id": "azbzYRkbvTHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debemos depositar las imágenes que queremos dentro de la carpeta input"
      ],
      "metadata": {
        "id": "NHjfp_3ezD3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# Ejecuta el comando nnUNetv2_predict para realizar predicciones en el dataset especificado.\n",
        "# La opción -d especifica el dataset.\n",
        "# La opción -i especifica el directorio de entrada.\n",
        "# La opción -o especifica el directorio de salida.\n",
        "# La opción -f especifica los pliegues (folds) a utilizar.\n",
        "# La opción -tr especifica el entrenador.\n",
        "# La opción -c especifica la configuración (3D en este caso).\n",
        "# La opción -p especifica el planificador a utilizar.\n",
        "\n",
        "!nnUNetv2_predict \\\n",
        "    -d Dataset001_Stroke \\\n",
        "    -i $work_nnUNet/test/input \\\n",
        "    -o $work_nnUNet/test/output \\\n",
        "    -f 0 1 2 3 4 \\\n",
        "    -tr nnUNetTrainer \\\n",
        "    -c 3d_fullres \\\n",
        "    -p nnUNetResEncUNetLPlans"
      ],
      "metadata": {
        "id": "bP8lX3Oly-O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente aplicamos el postprocesamiento"
      ],
      "metadata": {
        "id": "XGqxaNHczN7w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7smsU0aokR1m"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Ejecuta el comando nnUNetv2_apply_postprocessing para aplicar\n",
        "# postprocesamiento a las predicciones generadas.\n",
        "# La opción -i especifica el directorio de entrada donde se\n",
        "# encuentran las predicciones.\n",
        "# La opción -o especifica el directorio de salida donde se\n",
        "# guardarán los resultados postprocesados.\n",
        "# La opción -pp_pkl_file especifica el archivo .pkl que contiene\n",
        "# la configuración de postprocesamiento.\n",
        "# La opción -np especifica el número de procesos paralelos a\n",
        "# utilizar.\n",
        "# La opción -plans_json especifica el archivo JSON con los planes\n",
        "# utilizados durante el entrenamiento.\n",
        "\n",
        "nnUNetv2_apply_postprocessing \\\n",
        "    -i $work_nnUNet/test/output \\\n",
        "    -o $work_nnUNet/test/output_pp \\\n",
        "    -pp_pkl_file $nnUNet_results/Dataset001_Stroke/ \\\n",
        "    nnUNetTrainer__nnUNetResEncUNetLPlans__3d_fullres/ \\\n",
        "    crossval_results_folds_0_1_2_3_4/postprocessing.pkl \\\n",
        "    -np 8 \\\n",
        "    -plans_json $nnUNet_results/ \\\n",
        "    Dataset001_Stroke/nnUNetTrainer__nnUNetResEncUNetLPlans__3d_fullres/ \\\n",
        "    crossval_results_folds_0_1_2_3_4/plans.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Qsv5NmM-TAJL",
        "outputId": "f83a4156-5aeb-4cdf-d1bd-80ba4231ce41"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/data_norm.zip'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "outputfile = \"/content/drive/MyDrive/data_norm\"\n",
        "shutil.make_archive(outputfile, 'zip', \"/content/nnUNetFrame/dataset/nnUNet_raw/Dataset001_Stroke\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 4801596,
          "sourceId": 8125036,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30683,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}